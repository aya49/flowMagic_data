EncoderDecoder(
    (backbone): VIT_MLA(
        (patch_embed): PatchEmbed(
            (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
        )
        (pos_drop): Dropout(p=0.0, inplace=False)
        (blocks): ModuleList(
            (0): Block(
                (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (attn): Attention(
                    (qkv): Linear(in_features=1024, out_features=3072, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=1024, out_features=1024, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                )
                (drop_path): Identity()
                (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
                (mlp): Mlp(
                    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                )
            )
            ...
            (23): Block()
        )
        (mla): Conv_MLA(
            (mla_p2_1x1): Sequential(
                (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
            )
            ...
            (mla_p5_1x1): Sequential()
            (mla_p2): Sequential(
                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
            )
            ...
            (mla_p5): Sequential()
        )
        (norm_0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        ...
        (norm_3): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
    )
    (decode_head): VIT_MLAHead(
        input_transform=None, ignore_index=255, align_corners=False
        (loss_decode): CrossEntropyLoss()
        (conv_seg): Conv2d(512, 60, kernel_size=(1, 1), stride=(1, 1))
        (dropout): Dropout2d(p=0.1, inplace=False)
        (mlahead): MLAHead(
            (head2): Sequential(
                (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (2): ReLU()
                (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                (4): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (5): ReLU()
            )
            ...
            (head5): Sequential()
        )
        (cls): Conv2d(512, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (auxiliary_head): ModuleList(
        (0): VIT_MLA_AUXIHead(
            input_transform=None, ignore_index=255, align_corners=False
            (loss_decode): CrossEntropyLoss()
            (conv_seg): Conv2d(512, 60, kernel_size=(1, 1), stride=(1, 1))
            (dropout): Dropout2d(p=0.1, inplace=False)
            (aux): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)
        )
        ...
        (3): VIT_MLA_AUXIHead()
    )
)

